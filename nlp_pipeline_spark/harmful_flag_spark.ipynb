{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4e6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PySpark Setup Cell ---\n",
    "import os, sys, findspark\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = r\"E:\\Coding\\BDA-PySpark\\spark-3.4.1-bin-hadoop3\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8abbc9",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0177d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import threading\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064515b",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2159c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HDFS input directory\n",
    "hdfs_uri = \"hdfs://localhost:9000\"\n",
    "hdfs_filtered_dir = f\"{hdfs_uri}/user/adarsh/realtime_pipeline/filtered_batches\"\n",
    "\n",
    "# Path for saving the final CSV\n",
    "output_dir = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\results_spark\"\n",
    "final_output_file = os.path.join(output_dir, \"harmful_flags_spark.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f422b5",
   "metadata": {},
   "source": [
    "Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc34120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HarmfulFlagPipeline\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c40ed9",
   "metadata": {},
   "source": [
    "1. `run_harmful_udf` function:  RoBERTa model (SamLowe/roberta-base-go_emotions) to classify each text based on emotion\n",
    "2. Processes in batches and returns : `label` and `confidence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6a459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_harmful_udf(text):\n",
    "    local_model = threading.local()\n",
    "\n",
    "    def get_model():\n",
    "        if not hasattr(local_model, \"model\"):\n",
    "            device = 0 if torch.cuda.is_available() else -1\n",
    "            local_model.model = pipeline(\n",
    "                \"text-classification\",\n",
    "                model=\"SamLowe/roberta-base-go_emotions\",\n",
    "                framework=\"pt\",\n",
    "                device=device,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "        return local_model.model\n",
    "\n",
    "    try:\n",
    "        model = get_model()\n",
    "        if not isinstance(text, str) or len(text.strip().split()) < 5:\n",
    "            return {'label': 'neutral', 'score': 0.0}\n",
    "\n",
    "        text = text.strip()\n",
    "        words = text.split()\n",
    "        max_words = 400\n",
    "        if len(words) <= max_words:\n",
    "            chunks = [text]\n",
    "        else:\n",
    "            chunks = [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
    "\n",
    "        batch_size = 16\n",
    "        total_batches = (len(chunks) + batch_size - 1) // batch_size\n",
    "        chunk_preds = []\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch_num = i // batch_size + 1\n",
    "            batch = chunks[i:i + batch_size]\n",
    "            print(f\"Processing batch {batch_num} of {total_batches}\", file=sys.stderr)\n",
    "            preds = model(batch, truncation=True, max_length=512)\n",
    "            chunk_preds.extend(preds)\n",
    "\n",
    "        if not chunk_preds:\n",
    "            return {'label': 'neutral', 'score': 0.0}\n",
    "        if len(chunk_preds) == 1:\n",
    "            return chunk_preds[0]\n",
    "        label_weights = {}\n",
    "        for result in chunk_preds:\n",
    "            label = result['label']\n",
    "            score = result['score']\n",
    "            label_weights[label] = label_weights.get(label, 0.0) + score\n",
    "\n",
    "        final_label = max(label_weights, key=label_weights.get)\n",
    "        final_score = label_weights[final_label] / len(chunk_preds)\n",
    "        return {'label': final_label, 'score': final_score}\n",
    "    except:\n",
    "        return {'label': 'error', 'score': 0.0}\n",
    "\n",
    "\n",
    "result_schema = StructType([\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"score\", FloatType(), True)\n",
    "])\n",
    "\n",
    "harmful_pred_udf = udf(run_harmful_udf, result_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d2b459",
   "metadata": {},
   "source": [
    "recursive cleaning of temporary directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5675901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_single_csv(df, output_path):\n",
    "    temp_dir = output_path + \"_temp\"\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)  \n",
    "\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(temp_dir)\n",
    "\n",
    "    part_file = glob.glob(os.path.join(temp_dir, 'part-*.csv'))\n",
    "    if not part_file:\n",
    "        raise FileNotFoundError(\"No part file found in temp directory\")\n",
    "    part_file = part_file[0]\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "    shutil.move(part_file, output_path)\n",
    "    shutil.rmtree(temp_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bfab81",
   "metadata": {},
   "source": [
    "store and save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_save(df_spark):\n",
    "    df_result = df_spark.withColumn(\"harmful_result\", harmful_pred_udf(col(\"comment\")))\n",
    "    df_result = df_result.withColumn(\"harmful_label\", col(\"harmful_result.label\")) \\\n",
    "                         .withColumn(\"score\", col(\"harmful_result.score\")) \\\n",
    "                         .drop(\"harmful_result\")\n",
    "\n",
    "    save_single_csv(df_result, final_output_file)\n",
    "    print(f\"Output saved to {output_dir} as harmful_flags_spark.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84195065",
   "metadata": {},
   "source": [
    "Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd87431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from HDFS path: hdfs://localhost:9000/user/adarsh/realtime_pipeline/filtered_batches\n",
      "Read 322 rows.\n",
      "After filtering, 322 rows left.\n",
      "Starting harmful content detection UDF\n",
      "Output saved to {output_dir} as harmful_flags_spark.csv\n",
      "Completed harmful content detection.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"Reading data from HDFS path: {hdfs_filtered_dir}\")\n",
    "    df_spark = spark.read.text(hdfs_filtered_dir).toDF(\"comment\")\n",
    "    print(f\"Read {df_spark.count()} rows.\")\n",
    "\n",
    "    df_spark = df_spark.filter(\"length(comment) > 10\")\n",
    "    print(f\"After filtering, {df_spark.count()} rows left.\")\n",
    "\n",
    "    print(\"Starting harmful content detection UDF\")\n",
    "    run_and_save(df_spark)\n",
    "    print(\"Completed harmful content detection.\")\n",
    "\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark 3.10)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
