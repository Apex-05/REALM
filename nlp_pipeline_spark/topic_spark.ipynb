{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d25fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PySpark Setup Cell ---\n",
    "import os, sys, findspark\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = r\"E:\\Coding\\BDA-PySpark\\spark-3.4.1-bin-hadoop3\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e2bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "[INFO] Starting Topic Clustering Pipeline...\n",
      "===============================\n",
      "[INFO] Generating embeddings with batching...\n",
      "[INFO] Total rows with valid embeddings: 148\n",
      "[INFO] Running KMeans clustering...\n",
      "[INFO] Total clusters generated: 5\n",
      "\n",
      "[INFO] Cluster distribution:\n",
      "   Cluster 0: 7 samples\n",
      "   Cluster 1: 53 samples\n",
      "   Cluster 2: 33 samples\n",
      "   Cluster 3: 26 samples\n",
      "   Cluster 4: 29 samples\n",
      "[RESULT] Silhouette Score: 0.0736\n",
      "[INFO] Saving final results to: E:/Coding/BDA-PySpark/realtime-pipeline/results_spark/topic_results_spark.csv\n",
      "[SUCCESS] Clustering completed and results saved.\n",
      "===============================\n",
      "[INFO] Spark session stopped successfully.\n"
     ]
    }
   ],
   "source": [
    "# Optimized Topic Clustering with Spark + SentenceTransformer\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TopicClusteringPipelineOptimized\")\n",
    "    .config(\"spark.master\", \"local[*]\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")  \n",
    "    .config(\"spark.default.parallelism\", \"4\")\n",
    "    .config(\"spark.network.timeout\", \"1200s\")\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"200s\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Optimized embed_partition (batched)\n",
    "\n",
    "def embed_partition(iterator):\n",
    "    \"\"\"\n",
    "    Encode texts in batches per partition.\n",
    "    Loads model once per partition to avoid timeouts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "        batch_texts, batch_rows = [], []\n",
    "        batch_size = 32  \n",
    "\n",
    "        for row in iterator:\n",
    "            text = row[\"cleaned_text\"]\n",
    "            if text and isinstance(text, str) and len(text.strip()) > 0:\n",
    "                batch_rows.append(row)\n",
    "                batch_texts.append(text)\n",
    "\n",
    "            if len(batch_texts) >= batch_size:\n",
    "                embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
    "                for r, e in zip(batch_rows, embeddings):\n",
    "                    yield (r[\"comment\"], r[\"cleaned_text\"], e.tolist())\n",
    "                batch_texts, batch_rows = [], []\n",
    "\n",
    "        if batch_texts:\n",
    "            embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
    "            for r, e in zip(batch_rows, embeddings):\n",
    "                yield (r[\"comment\"], r[\"cleaned_text\"], e.tolist())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR in partition]: {e}\")\n",
    "        return\n",
    "\n",
    "# Schema\n",
    "embed_schema = StructType([\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"cleaned_text\", StringType(), True),\n",
    "    StructField(\"features_array\", ArrayType(FloatType()), True)\n",
    "])\n",
    "\n",
    "# 4. Main Pipeline\n",
    "def run_spark_kmeans_pipeline(spark, input_path, output_path):\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"[INFO] Starting Topic Clustering Pipeline...\")\n",
    "    print(\"===============================\")\n",
    "\n",
    "    df = spark.read.text(input_path).withColumnRenamed(\"value\", \"comment\")\n",
    "    df = df.withColumn(\"cleaned_text\", F.trim(F.lower(F.col(\"comment\"))))\n",
    "    df = df.filter(F.length(\"cleaned_text\") > 10)\n",
    "\n",
    "    if df.count() == 0:\n",
    "        print(\"[ERROR] No valid rows found.\")\n",
    "        return\n",
    "\n",
    "    df = df.coalesce(4)   \n",
    "\n",
    "    \n",
    "    print(\"[INFO] Generating embeddings with batching...\")\n",
    "    rdd = df.rdd.mapPartitions(embed_partition)\n",
    "    df_embed = spark.createDataFrame(rdd, schema=embed_schema)\n",
    "\n",
    "   \n",
    "    array_to_vector_udf = F.udf(lambda arr: Vectors.dense(arr) if arr else None, VectorUDT())\n",
    "    df_features = df_embed.withColumn(\"features\", array_to_vector_udf(\"features_array\")).cache()\n",
    "\n",
    "    valid_count = df_features.count()\n",
    "    print(f\"[INFO] Total rows with valid embeddings: {valid_count}\")\n",
    "    if valid_count < 2:\n",
    "        print(\"[ERROR] Not enough embeddings for clustering.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    # KMeans clustering\n",
    "    \n",
    "    print(\"[INFO] Running KMeans clustering...\")\n",
    "    k = 5\n",
    "    kmeans = KMeans(k=k, seed=42, featuresCol=\"features\", predictionCol=\"topic\")\n",
    "    model = kmeans.fit(df_features)\n",
    "    clustered = model.transform(df_features)\n",
    "\n",
    "    centroids = model.clusterCenters()\n",
    "    num_clusters = len(centroids)\n",
    "    print(f\"[INFO] Total clusters generated: {num_clusters}\")\n",
    "\n",
    "    cluster_sizes = clustered.groupBy(\"topic\").count().orderBy(\"topic\").collect()\n",
    "    print(\"\\n[INFO] Cluster distribution:\")\n",
    "    for row in cluster_sizes:\n",
    "        print(f\"   Cluster {row['topic']}: {row['count']} samples\")\n",
    "\n",
    "    # Evaluate clustering quality\n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"topic\", metricName=\"silhouette\")\n",
    "    silhouette = evaluator.evaluate(clustered)\n",
    "    print(f\"[RESULT] Silhouette Score: {silhouette:.4f}\")\n",
    "\n",
    "    # Compute topic probabilities (cosine similarity)\n",
    "    centroids_arr = [np.array(c) for c in centroids]\n",
    "    centroid_bcast = spark.sparkContext.broadcast(centroids_arr)\n",
    "\n",
    "    def cosine_similarity(v, idx):\n",
    "        if v is None or idx is None:\n",
    "            return 0.0\n",
    "        v = np.array(v)\n",
    "        c = centroid_bcast.value[int(idx)]\n",
    "        return float(np.dot(v, c) / (np.linalg.norm(v) * np.linalg.norm(c)))\n",
    "\n",
    "    cosine_udf = F.udf(cosine_similarity, FloatType())\n",
    "\n",
    "    df_final = (\n",
    "        clustered\n",
    "        .withColumn(\"topic_prob\", cosine_udf(F.col(\"features_array\"), F.col(\"topic\")))\n",
    "        .select(\"comment\", \"topic\", \"topic_prob\")\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"[INFO] Saving final results to: {output_path}\")\n",
    "    df_final.toPandas().to_csv(output_path, index=False)\n",
    "    print(\"[SUCCESS] Clustering completed and results saved.\")\n",
    "    print(\"===============================\")\n",
    "\n",
    "\n",
    "\n",
    "hdfs_uri = \"hdfs://localhost:9000\"\n",
    "hdfs_raw_dir = f\"{hdfs_uri}/user/adarsh/realtime_pipeline/raw_batches\"\n",
    "topics_path = \"E:/Coding/BDA-PySpark/realtime-pipeline/results_spark/\"\n",
    "os.makedirs(topics_path, exist_ok=True)\n",
    "final_output_file = os.path.join(topics_path, \"topic_results_spark.csv\")\n",
    "\n",
    "run_spark_kmeans_pipeline(spark, hdfs_raw_dir, final_output_file)\n",
    "\n",
    "spark.stop()\n",
    "print(\"[INFO] Spark session stopped successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20057b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf6d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7dbb14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark 3.10)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
