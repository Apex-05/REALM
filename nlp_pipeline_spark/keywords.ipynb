{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96b8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rake_nltk import Rake\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import subprocess\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6dd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "\n",
    "input_path = r\"hdfs://localhost:9000/user/adarsh/realtime_pipeline/raw_batches\"\n",
    "local_fallback_path = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\raw_batches\"  \n",
    "hadoop_bin = r\"E:\\hadoop\\bin\\hdfs.cmd\" \n",
    "\n",
    "output_dir = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\results_spark\"\n",
    "final_output_file = os.path.join(output_dir, \"keywords_spark.csv\")\n",
    "\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "215677a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER: READ FILES FROM HDFS OR LOCAL\n",
    "\n",
    "def read_hdfs_texts(hdfs_path):\n",
    "   \n",
    "    try:\n",
    "        print(f\"Attempting to read from HDFS: {hdfs_path}\")\n",
    "        result = subprocess.run(\n",
    "            [hadoop_bin, \"dfs\", \"-cat\", f\"{hdfs_path}/*.txt\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        lines = [line.strip() for line in result.stdout.split(\"\\n\") if line.strip()]\n",
    "        print(f\"Loaded {len(lines)} comments from HDFS.\")\n",
    "        return lines\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Hadoop CLI not found. Falling back to local folder..\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error reading from HDFS: {e}. Falling back to local folder..\")\n",
    "\n",
    "    # Local Fallback \n",
    "    local_files = glob.glob(os.path.join(local_fallback_path, \"*.txt\"))\n",
    "    if not local_files:\n",
    "        print(\"No local batch files found either.\")\n",
    "        return []\n",
    "    all_lines = []\n",
    "    for f in local_files:\n",
    "        with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as infile:\n",
    "            all_lines.extend([line.strip() for line in infile if line.strip()])\n",
    "    print(f\"Loaded {len(all_lines)} comments from local batches.\")\n",
    "    return all_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8184a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAKE + TF-IDF SETUP\n",
    "RAKE = Rake(min_length=1, max_length=4, include_repeated_phrases=False)\n",
    "\n",
    "# Custom stopwords\n",
    "CUSTOM_STOPWORDS = {\n",
    "    'i','me','my','mine','you','your','yours','he','him','his','she','her','hers','it','its','we','us','our','ours',\n",
    "    'they','them','their','theirs','a','an','the','and','but','or','if','because','as','while','of','at','by','for',\n",
    "    'with','about','against','between','into','through','during','before','after','above','below','to','from','up',\n",
    "    'down','in','out','on','off','over','under','what','where','when','who','whom','which','why','how',\n",
    "    'hi','hello','hey','thanks','thank','please','welcome','sure','yeah','ok','okay','alright','fine','right','well',\n",
    "    'oh','hmm','ah','uh','um','huh','wow','oops','hahaha','haha','lol','really','very','just','actually','maybe',\n",
    "    'probably','basically','literally','clearly','obviously','definitely','simply','honestly','apparently','anyway',\n",
    "    'somehow','sometimes','often','always','lot','lots','thing','things','stuff','something','anything','everything',\n",
    "    'nothing','some','many','much','more','less','few','kind','kinda','sort','sorta','type','types','group','bunch',\n",
    "    'can','cannot','could','should','would','may','might','must','shall','will','did','does','doing','done','make',\n",
    "    'makes','made','take','takes','taken','say','says','said','see','seen','seems','know','knows','knew','think',\n",
    "    'thought','want','wants','wanted','go','goes','went','gone','get','gets','got','gotten','come','came','coming',\n",
    "    'put','puts','putting','try','trying','tried','use','used','using','feel','feels','felt','give','gave','given',\n",
    "    'edit','deleted','removed','update','reddit','comment','post','thread','user','bot','mod','mods','report','flair',\n",
    "    'image','link','url','amp','nbsp','lt','gt','http','https','com'\n",
    "}\n",
    "\n",
    "STOPWORDS = list(ENGLISH_STOP_WORDS.union(CUSTOM_STOPWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4524d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEYWORD EXTRACTION FUNCTIONS\n",
    "def filter_topic_keywords(ranked_phrases_with_scores, min_score=1.0):\n",
    "    filtered = []\n",
    "    for item in ranked_phrases_with_scores:\n",
    "        if isinstance(item, tuple) and len(item) == 2:\n",
    "            score, phrase = item\n",
    "            phrase_words = phrase.lower().split()\n",
    "            if isinstance(score, float) and score >= min_score and all(w not in STOPWORDS for w in phrase_words):\n",
    "                filtered.append(phrase)\n",
    "        elif isinstance(item, str):\n",
    "            phrase_words = item.lower().split()\n",
    "            if all(w not in STOPWORDS for w in phrase_words):\n",
    "                filtered.append(item)\n",
    "    return filtered[:10]\n",
    "\n",
    "def extract_tfidf_keywords(texts, n_keywords=5):\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            stop_words=STOPWORDS,\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2,\n",
    "            max_df=0.8\n",
    "        )\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        avg_scores = np.asarray(X.mean(axis=0)).flatten()\n",
    "        top_indices = avg_scores.argsort()[::-1][:n_keywords]\n",
    "        return [feature_names[i] for i in top_indices]\n",
    "    except Exception as e:\n",
    "        print(f\"TF-IDF extraction error: {e}\")\n",
    "        return []\n",
    "\n",
    "def run_keyword_extraction(comments, use_tfidf=True):\n",
    "    result_comments, result_keywords = [], []\n",
    "\n",
    "    for text in comments:\n",
    "        text = text.strip()\n",
    "        if not text or len(text.split()) < 5:\n",
    "            continue\n",
    "        try:\n",
    "            RAKE.extract_keywords_from_text(text)\n",
    "            ranked_phrases = RAKE.get_ranked_phrases_with_scores()\n",
    "            topic_keywords = filter_topic_keywords(ranked_phrases)\n",
    "            topic_keywords = [kw.replace(\",\", \"\") for kw in topic_keywords]\n",
    "            keywords_str = \" \".join(topic_keywords) if topic_keywords else \"NoKeywords\"\n",
    "            result_comments.append(text)\n",
    "            result_keywords.append(keywords_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting keywords: {e}\")\n",
    "            result_comments.append(text)\n",
    "            result_keywords.append(\"Error processing\")\n",
    "\n",
    "    df = pd.DataFrame({\"comment\": result_comments, \"keywords\": result_keywords})\n",
    "\n",
    "    # Compute summary\n",
    "    all_keywords = []\n",
    "    for kw_str in result_keywords:\n",
    "        if kw_str and kw_str not in [\"No keywords found\", \"Error processing\"]:\n",
    "            all_keywords.extend(kw_str.split())\n",
    "    summary = pd.Series(all_keywords).value_counts() if all_keywords else pd.Series(dtype=int)\n",
    "    return df, summary\n",
    "\n",
    "def extract_topic_themes(df, n_topics=5):\n",
    "    \"\"\"Return the most frequent keywords (topic themes).\"\"\"\n",
    "    all_keywords = []\n",
    "    for kw_str in df['keywords']:\n",
    "        if kw_str and kw_str not in [\"No keywords found\", \"Error processing\"]:\n",
    "            all_keywords.extend(kw_str.split())\n",
    "    keyword_counts = Counter(all_keywords)\n",
    "    top_themes = keyword_counts.most_common(n_topics)\n",
    "    return top_themes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "785b72b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting keyword extraction pipeline...\n",
      "Attempting to read from HDFS: hdfs://localhost:9000/user/adarsh/realtime_pipeline/raw_batches\n",
      "Loaded 134 comments from HDFS.\n",
      "Keywords saved to: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\results_spark\\keywords_spark.csv\n"
     ]
    }
   ],
   "source": [
    "# MAIN EXECUTION\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting keyword extraction pipeline...\")\n",
    "\n",
    "    comments = read_hdfs_texts(input_path)\n",
    "    if not comments:\n",
    "        print(\"No data found in HDFS or local path.\")\n",
    "        exit(0)\n",
    "\n",
    "    df_keywords, summary = run_keyword_extraction(comments, use_tfidf=True)\n",
    "\n",
    "    # Save results\n",
    "    df_keywords.to_csv(final_output_file, index=False, encoding='utf-8')\n",
    "    print(f\"Keywords saved to: {final_output_file}\")\n",
    "\n",
    "    # Display top topic themes\n",
    "    extract_topic_themes(df_keywords, n_topics=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark 3.10)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
