{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b92c46b",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74c0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import re\n",
    "import praw\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import sys, findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e47bf85",
   "metadata": {},
   "source": [
    "# NLTK (Natural Language Toolkit)  Setup\n",
    "1. `words` corpus is downloaded to use a list of valid English words for keyword validation, filtering, or cleaning operations.\n",
    "2. corpus is converted into a lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b54f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Adarsh\n",
      "[nltk_data]     Ranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('words')\n",
    "english_words = set(w.lower() for w in words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1fa51",
   "metadata": {},
   "source": [
    "# PATHS\n",
    "1. Local + HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809e624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local directories\n",
    "raw_batches = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\raw_batches\"\n",
    "filtered_batches = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\filtered_batches\"\n",
    "os.makedirs(raw_batches, exist_ok=True)\n",
    "os.makedirs(filtered_batches, exist_ok=True)\n",
    "\n",
    "# HDFS directories\n",
    "hdfs_uri = \"hdfs://localhost:9000\"\n",
    "hdfs_raw_dir = f\"{hdfs_uri}/user/adarsh/realtime_pipeline/raw_batches\"\n",
    "hdfs_filtered_dir = f\"{hdfs_uri}/user/adarsh/realtime_pipeline/filtered_batches\"\n",
    "\n",
    "HDFS_CMD = r\"E:\\hadoop\\bin\\hdfs.cmd\"\n",
    "\n",
    "# Spark checkpoint\n",
    "checkpoint_dir = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\checkpoints\"\n",
    "\n",
    "os.environ['HADOOP_USER_NAME'] = 'AdarshRanjan'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e9385",
   "metadata": {},
   "source": [
    "# Reddit API Connection\n",
    "1. Uses PRAW (Python Reddit API Wrapper) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e173ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reddit API connected.\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id = \"ID\",\n",
    "    client_secret = \"secret\", \n",
    "    user_agent=\"user\"\n",
    ")\n",
    "print(\" Reddit API connected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0a66f",
   "metadata": {},
   "source": [
    "# Data and Log Cleanup Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "702cbb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOCAL] Skipped deletion of local files in E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\raw_batches\n",
      "[HDFS] Skipped deletion of HDFS folder hdfs://localhost:9000/user/adarsh/realtime_pipeline/raw_batches\n",
      "[LOCAL] Ensured local folder exists: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\raw_batches\n",
      "[LOCAL] Created placeholder file: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\raw_batches\\Placeholder.txt\n",
      "[HDFS] Ensured HDFS folder exists: hdfs://localhost:9000/user/adarsh/realtime_pipeline/raw_batches\n",
      "[HDFS] Uploaded placeholder file to HDFS: hdfs://localhost:9000/user/adarsh/realtime_pipeline/raw_batches\n",
      "[LOCAL] Skipped deletion of local files in E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\filtered_batches\n",
      "[HDFS] Skipped deletion of HDFS folder hdfs://localhost:9000/user/adarsh/realtime_pipeline/filtered_batches\n",
      "[LOCAL] Ensured local folder exists: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\filtered_batches\n",
      "[LOCAL] Created placeholder file: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\filtered_batches\\Placeholder.txt\n",
      "[HDFS] Ensured HDFS folder exists: hdfs://localhost:9000/user/adarsh/realtime_pipeline/filtered_batches\n",
      "[HDFS] Uploaded placeholder file to HDFS: hdfs://localhost:9000/user/adarsh/realtime_pipeline/filtered_batches\n",
      "[LOCAL] Skipped deletion (no recreation)\n",
      "\n",
      " Cleanup and placeholder setup complete.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "log = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\logs\"\n",
    "\n",
    "\n",
    "\n",
    "confirm = input(\"This will delete local batch files, logs, checkpoints, and HDFS data. Continue? (y/n): \")\n",
    "\n",
    "for local_dir, hdfs_dir in [(raw_batches, hdfs_raw_dir), (filtered_batches, hdfs_filtered_dir)]:\n",
    "    # Local batch cleanup\n",
    "    if confirm.lower() == 'y' and os.path.exists(local_dir):\n",
    "        deleted_files = 0\n",
    "        for f in os.listdir(local_dir):\n",
    "            if (f.startswith(\"r_batch\") or f.startswith(\"f_batch\")) and f.endswith(\".txt\"):\n",
    "                os.remove(os.path.join(local_dir, f))\n",
    "                deleted_files += 1\n",
    "        print(f\"[LOCAL] Deleted {deleted_files} old batch files in {local_dir}\")\n",
    "    else:\n",
    "        print(f\"[LOCAL] Skipped deletion of local files in {local_dir}\")\n",
    "\n",
    "    # HDFS cleanup \n",
    "    if confirm.lower() == 'y':\n",
    "        try:\n",
    "            subprocess.run([HDFS_CMD, \"dfs\", \"-rm\", \"-r\", \"-skipTrash\", hdfs_dir],\n",
    "                           check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            print(f\"[HDFS] Deleted existing HDFS folder {hdfs_dir}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"[HDFS] HDFS folder {hdfs_dir} did not exist or could not be deleted\")\n",
    "    else:\n",
    "        print(f\"[HDFS] Skipped deletion of HDFS folder {hdfs_dir}\")\n",
    "\n",
    "    # Ensure local folder exists \n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    print(f\"[LOCAL] Ensured local folder exists: {local_dir}\")\n",
    "\n",
    "    # Create placeholder file \n",
    "    placeholder_file = os.path.join(local_dir, \"Placeholder.txt\")\n",
    "    with open(placeholder_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"placeholder\\n\")\n",
    "    print(f\"[LOCAL] Created placeholder file: {placeholder_file}\")\n",
    "\n",
    "    # Ensure HDFS folder exists \n",
    "    subprocess.run([HDFS_CMD, \"dfs\", \"-mkdir\", \"-p\", hdfs_dir], check=True)\n",
    "    print(f\"[HDFS] Ensured HDFS folder exists: {hdfs_dir}\")\n",
    "\n",
    "    # Upload placeholder to HDFS\n",
    "    subprocess.run([HDFS_CMD, \"dfs\", \"-put\", \"-f\", placeholder_file, hdfs_dir], check=True)\n",
    "    print(f\"[HDFS] Uploaded placeholder file to HDFS: {hdfs_dir}\")\n",
    "\n",
    "# Clean up logs \n",
    "if confirm.lower() == 'y':\n",
    "    if os.path.exists(log):\n",
    "        try:\n",
    "            shutil.rmtree(log)\n",
    "            print(f\"[LOCAL] Deleted entire logs directory: {log}\")\n",
    "            \n",
    "            # Recreate empty directory\n",
    "            os.makedirs(log, exist_ok=True)\n",
    "            print(f\"[LOCAL] Recreated empty logs directory: {log}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[LOCAL] Failed to delete logs directory: {e}\")\n",
    "    else:\n",
    "        print(f\"[LOCAL] Logs directory doesn't exist: {log}\")\n",
    "else:\n",
    "    print(f\"[LOCAL] Skipped deletion (no recreation)\")\n",
    "\n",
    "print(\"\\n Cleanup and placeholder setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd70b0b",
   "metadata": {},
   "source": [
    "# Emoji Replacement\n",
    "1. Converts emojis in words emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c634b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {\n",
    "    \"😂\": \"joy\", \"🤣\": \"laugh\", \"😊\": \"happy\", \"😄\": \"smile\", \"😁\": \"grin\", \"😆\": \"laugh\",\n",
    "    \"😃\": \"smile\", \"😎\": \"cool\", \"👍\": \"like\", \"❤️\": \"love\", \"💖\": \"love\", \"💯\": \"great\",\n",
    "\n",
    "    \"😢\": \"sad\", \"😭\": \"cry\", \"🙁\": \"sad\", \"😔\": \"disappointed\", \"😟\": \"worried\", \"😞\": \"sad\",\n",
    "    \"😩\": \"tired\", \"😫\": \"tired\",\n",
    "\n",
    "    \"😡\": \"angry\", \"😠\": \"angry\", \"👿\": \"angry\", \"💀\": \"death\", \"🔥\": \"fire\", \"⚡\": \"shock\",\n",
    "\n",
    "    \"😍\": \"love\", \"😘\": \"kiss\", \"🥰\": \"love\", \"💘\": \"love\", \"💓\": \"love\", \"💝\": \"love\",\n",
    "\n",
    "    \"😲\": \"surprise\", \"😮\": \"surprise\", \"😳\": \"shocked\", \"😱\": \"shocked\", \"🤯\": \"mindblown\",\n",
    "\n",
    "    \"😴\": \"sleepy\", \"💤\": \"sleep\", \"😌\": \"relieved\", \"😪\": \"sleepy\", \"😇\": \"innocent\",\n",
    "\n",
    "    \"👏\": \"clap\", \"🤝\": \"handshake\", \"✨\": \"sparkle\", \"🌟\": \"star\", \"🎉\": \"celebration\", \"🎊\": \"celebration\",\n",
    "\n",
    "    \"👎\": \"dislike\", \"💔\": \"heartbroken\", \"☹️\": \"sad\", \"🤢\": \"disgust\", \"🤮\": \"disgust\",\n",
    "\n",
    "    \"🤔\": \"thinking\", \"🙄\": \"eyeroll\", \"🤷\": \"shrug\", \"😐\": \"neutral\", \"😶\": \"silent\", \"😬\": \"nervous\"\n",
    "}\n",
    "\n",
    "def replace_emojis(text):\n",
    "    for emoji, meaning in emoji_dict.items():\n",
    "        text = text.replace(emoji, f\" {meaning} \")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0e6f9",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196077a",
   "metadata": {},
   "source": [
    "`clean_raw_text`: For Topic Modeling\\\n",
    "pre-processing for raw batch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808d27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stop words list\n",
    "stop_words = {\n",
    "    \"a\", \"an\", \"the\", \"and\", \"or\", \"is\", \"are\", \"to\", \"of\", \"in\", \"that\", \"this\",\n",
    "    \"it\", \"on\", \"for\", \"with\", \"as\", \"was\", \"at\", \"by\", \"be\", \"from\", \"has\", \"have\",\n",
    "    \"u\", \"im\", \"yeah\", \"oh\"\n",
    "}\n",
    "\n",
    "def clean_raw_text(text, min_words=50, chunk_size=150):\n",
    "    \n",
    "    original_text = text\n",
    "    \n",
    "    text = replace_emojis(text)\n",
    "    \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[@#]\\w+\", \"\", text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^a-z\\s.,!?'-]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    text = re.sub(r\"(\\w)'m(\\w)\", r\"\\1m\\2\", text)  \n",
    "    text = re.sub(r\"(\\w)'(\\w)\", r\"\\1\\2\", text)    \n",
    "    \n",
    "    text = text.replace(\"dont\", \"don't\")\n",
    "    text = text.replace(\"cant\", \"can't\")\n",
    "    text = text.replace(\"wont\", \"won't\")\n",
    "    text = text.replace(\"didnt\", \"didn't\")\n",
    "    text = text.replace(\"doesnt\", \"doesn't\")\n",
    "    text = text.replace(\"im\", \"i'm\")\n",
    "    text = text.replace(\"ive\", \"i've\")\n",
    "    text = text.replace(\"youre\", \"you're\")\n",
    "    text = text.replace(\"theyre\", \"they're\")\n",
    "    text = text.replace(\"thats\", \"that's\")\n",
    "    \n",
    "    \n",
    "    words_list = [\n",
    "        w for w in text.split()\n",
    "        if len(w) > 1  \n",
    "    ]\n",
    "    \n",
    "    if len(words_list) < min_words:\n",
    "        return None  \n",
    "    \n",
    "    return [\" \".join(words_list)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f46670",
   "metadata": {},
   "source": [
    "`clean_filtered_text`: For Sentiment Analysis\\\n",
    "pre-processing for filtered batch files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ab2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filtered_text(text, min_words=30):\n",
    "   \n",
    "    text = text.strip()\n",
    "    \n",
    "    \n",
    "    text = replace_emojis(text)\n",
    "    \n",
    "  \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"/[ru]/\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[@#]\\w+\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"(.)\\1{3,}\", r\"\\1\\1\", text)  \n",
    "    \n",
    "    text = text.replace(\"dont\", \"don't\")\n",
    "    text = text.replace(\"cant\", \"can't\")\n",
    "    text = text.replace(\"wont\", \"won't\")\n",
    "    text = text.replace(\"didnt\", \"didn't\")\n",
    "    text = text.replace(\"doesnt\", \"doesn't\")\n",
    "    text = text.replace(\"isnt\", \"isn't\")\n",
    "    text = text.replace(\"arent\", \"aren't\")\n",
    "    text = text.replace(\"wasnt\", \"wasn't\")\n",
    "    text = text.replace(\"werent\", \"weren't\")\n",
    "    text = text.replace(\"hasnt\", \"hasn't\")\n",
    "    text = text.replace(\"havent\", \"haven't\")\n",
    "    text = text.replace(\"hadnt\", \"hadn't\")\n",
    "    text = text.replace(\"wouldnt\", \"wouldn't\")\n",
    "    text = text.replace(\"shouldnt\", \"shouldn't\")\n",
    "    text = text.replace(\"couldnt\", \"couldn't\")\n",
    "    text = text.replace(\"im\", \"i'm\")\n",
    "    text = text.replace(\"ive\", \"i've\")\n",
    "    text = text.replace(\"youre\", \"you're\")\n",
    "    text = text.replace(\"youve\", \"you've\")\n",
    "    text = text.replace(\"theyre\", \"they're\")\n",
    "    text = text.replace(\"theyve\", \"they've\")\n",
    "    text = text.replace(\"its\", \"it's\")\n",
    "    text = text.replace(\"thats\", \"that's\")\n",
    "    text = text.replace(\"whats\", \"what's\")\n",
    "    text = text.replace(\"heres\", \"here's\")\n",
    "    text = text.replace(\"theres\", \"there's\")\n",
    "    \n",
    "    text = re.sub(r\"(\\w)'(\\w)\", r\"\\1\\2\", text)  \n",
    "    \n",
    "    text = re.sub(r\"[^\\w\\s.,!?'-]\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    word_count = len(text.split())\n",
    "    if word_count < min_words:\n",
    "        return None  \n",
    "    return [text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ebc026",
   "metadata": {},
   "source": [
    "`get_next_batch_number`\n",
    "1. Dynamically identifies the next available batch number based on existing files.\n",
    "2. Ensures sequential consistent naming like r_batch1.txt, r_batch2.txt, etc.\n",
    "\n",
    "`append_log_to_csv`\n",
    "1. Logs each uploaded batch's metadata to a CSV file (timestamp, file size, duration, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ea9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_next_batch_number(folder, prefix=\"batch\", suffix=\".txt\"):\n",
    "    \"\"\"Get the next batch number dynamically based on existing files.\"\"\"\n",
    "    existing = [f for f in os.listdir(folder) if f.startswith(prefix) and f.endswith(suffix)]\n",
    "    if not existing:\n",
    "        return 1\n",
    "    numbers = []\n",
    "    for f in existing:\n",
    "        try:\n",
    "            num = int(re.findall(r'\\d+', f)[0])\n",
    "            numbers.append(num)\n",
    "        except:\n",
    "            continue\n",
    "    return max(numbers) + 1\n",
    "\n",
    "\n",
    "def append_log_to_csv(log_entry):\n",
    "    \"\"\"Append single log entry to CSV\"\"\"\n",
    "    log_csv_path = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\logs\\upload_logs.csv\"\n",
    "    \n",
    "    os.makedirs(os.path.dirname(log_csv_path), exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(log_csv_path):\n",
    "        df = pd.DataFrame([log_entry])\n",
    "        df.to_csv(log_csv_path, index=False)\n",
    "    else:\n",
    "        df = pd.DataFrame([log_entry])\n",
    "        df.to_csv(log_csv_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef7220",
   "metadata": {},
   "source": [
    "# Reddit Streaming Thread (Heart of the pipeline)\n",
    "1. Continuously fetches new comments from Reddit’s public “all” subreddit.\n",
    "2. Cleans each comment using both pipelines (clean_raw_text and clean_filtered_text).\n",
    "3. Buffers cleaned text until either:\\\n",
    "    (a). 50 comments are collected, or\\\n",
    "    (b). a time threshold (e.g., 70–90 seconds) is reached.\n",
    "4. Writes data to batch files (one raw and one filtered).\n",
    "5. Uploads the files to the corresponding HDFS directories.\n",
    "6. Records details of each upload in a CSV log.\n",
    "7. Runs continuously in a background thread (daemon=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759beb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-29 06:40:26] Uploaded f_batch1.txt to HDFS folder: filtered_batches | Lines: 14\n",
      "[2025-10-29 06:40:46] Uploaded r_batch1.txt to HDFS folder: raw_batches | Lines: 6\n",
      "[2025-10-29 06:41:37] Uploaded f_batch2.txt to HDFS folder: filtered_batches | Lines: 26\n",
      "[2025-10-29 06:42:17] Uploaded r_batch2.txt to HDFS folder: raw_batches | Lines: 11\n",
      "[2025-10-29 06:42:47] Uploaded f_batch3.txt to HDFS folder: filtered_batches | Lines: 13\n",
      "[2025-10-29 06:43:48] Uploaded r_batch3.txt to HDFS folder: raw_batches | Lines: 10\n",
      "[2025-10-29 06:44:02] Uploaded f_batch4.txt to HDFS folder: filtered_batches | Lines: 17\n",
      "[2025-10-29 06:45:16] Uploaded f_batch5.txt to HDFS folder: filtered_batches | Lines: 17\n",
      "[2025-10-29 06:45:23] Uploaded r_batch4.txt to HDFS folder: raw_batches | Lines: 10\n",
      "[2025-10-29 06:46:26] Uploaded f_batch6.txt to HDFS folder: filtered_batches | Lines: 13\n",
      "[2025-10-29 06:46:53] Uploaded r_batch5.txt to HDFS folder: raw_batches | Lines: 10\n",
      "[2025-10-29 06:47:33] Uploaded f_batch7.txt to HDFS folder: filtered_batches | Lines: 22\n",
      "[2025-10-29 06:48:20] Uploaded r_batch6.txt to HDFS folder: raw_batches | Lines: 11\n",
      "[2025-10-29 06:48:43] Uploaded f_batch8.txt to HDFS folder: filtered_batches | Lines: 16\n",
      "[2025-10-29 06:49:51] Uploaded r_batch7.txt to HDFS folder: raw_batches | Lines: 12\n",
      "[2025-10-29 06:49:54] Uploaded f_batch9.txt to HDFS folder: filtered_batches | Lines: 12\n",
      "[2025-10-29 06:51:05] Uploaded f_batch10.txt to HDFS folder: filtered_batches | Lines: 12\n",
      "[2025-10-29 06:51:23] Uploaded r_batch8.txt to HDFS folder: raw_batches | Lines: 5\n",
      "[2025-10-29 06:52:15] Uploaded f_batch11.txt to HDFS folder: filtered_batches | Lines: 16\n",
      "[2025-10-29 06:52:53] Uploaded r_batch9.txt to HDFS folder: raw_batches | Lines: 10\n",
      "[2025-10-29 06:53:26] Uploaded f_batch12.txt to HDFS folder: filtered_batches | Lines: 15\n",
      "[2025-10-29 06:54:23] Uploaded r_batch10.txt to HDFS folder: raw_batches | Lines: 13\n",
      "[2025-10-29 06:54:36] Uploaded f_batch13.txt to HDFS folder: filtered_batches | Lines: 19\n",
      "[2025-10-29 06:55:47] Uploaded f_batch14.txt to HDFS folder: filtered_batches | Lines: 9\n",
      "[2025-10-29 06:55:54] Uploaded r_batch11.txt to HDFS folder: raw_batches | Lines: 5\n",
      "[2025-10-29 06:56:57] Uploaded f_batch15.txt to HDFS folder: filtered_batches | Lines: 16\n",
      "[2025-10-29 06:57:24] Uploaded r_batch12.txt to HDFS folder: raw_batches | Lines: 12\n",
      "[2025-10-29 06:58:07] Uploaded f_batch16.txt to HDFS folder: filtered_batches | Lines: 20\n",
      "[2025-10-29 06:58:55] Uploaded r_batch13.txt to HDFS folder: raw_batches | Lines: 11\n",
      "[2025-10-29 06:59:18] Uploaded f_batch17.txt to HDFS folder: filtered_batches | Lines: 13\n",
      "[2025-10-29 07:00:26] Uploaded r_batch14.txt to HDFS folder: raw_batches | Lines: 6\n",
      "[2025-10-29 07:00:29] Uploaded f_batch18.txt to HDFS folder: filtered_batches | Lines: 11\n",
      "[2025-10-29 07:01:39] Uploaded f_batch19.txt to HDFS folder: filtered_batches | Lines: 20\n",
      "[2025-10-29 07:01:56] Uploaded r_batch15.txt to HDFS folder: raw_batches | Lines: 15\n",
      "[2025-10-29 07:02:50] Uploaded f_batch20.txt to HDFS folder: filtered_batches | Lines: 20\n"
     ]
    }
   ],
   "source": [
    "buffer_raw, buffer_filtered = [], []\n",
    "last_write_time_raw, last_write_time_filtered = time.time(), time.time()\n",
    "batch_count_raw = get_next_batch_number(raw_batches, prefix=\"r_batch\")\n",
    "batch_count_filtered = get_next_batch_number(filtered_batches, prefix=\"f_batch\")\n",
    "\n",
    "def stream_reddit_comments():\n",
    "    global buffer_raw, buffer_filtered\n",
    "    global last_write_time_raw, last_write_time_filtered\n",
    "    global batch_count_raw, batch_count_filtered\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            for comment in reddit.subreddit(\"all\").stream.comments(skip_existing=True):\n",
    "                text = comment.body.replace(\"\\n\", \" \").strip()\n",
    "                \n",
    "                lines_raw = clean_raw_text(text)\n",
    "                lines_filtered = clean_filtered_text(text)\n",
    "\n",
    "                if lines_raw:\n",
    "                    buffer_raw.extend(lines_raw)\n",
    "                if lines_filtered:\n",
    "                    buffer_filtered.extend(lines_filtered)\n",
    "\n",
    "                current_time = time.time()\n",
    "\n",
    "                if len(buffer_raw) >= 50 or (current_time - last_write_time_raw >= 90):\n",
    "                    if buffer_raw:\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        filename = f\"r_batch{batch_count_raw}.txt\"\n",
    "                        local_path = os.path.join(raw_batches, filename)\n",
    "                        \n",
    "                       \n",
    "                        with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(\"\\n\\n\".join(buffer_raw))  \n",
    "\n",
    "                        \n",
    "                       \n",
    "                        file_size = os.path.getsize(local_path)\n",
    "                        \n",
    "                      \n",
    "                        subprocess.run([\n",
    "                            r\"E:\\hadoop\\bin\\hdfs.cmd\", \"dfs\", \"-put\", \"-f\", local_path, hdfs_raw_dir\n",
    "                        ], check=True)\n",
    "                        \n",
    "                        \n",
    "                        duration = time.time() - start_time\n",
    "                        \n",
    "                        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        print(f\"[{timestamp}] Uploaded {filename} to HDFS folder: raw_batches | Lines: {len(buffer_raw)}\")\n",
    "                        \n",
    "                        log_entry = {\n",
    "                            \"timestamp\": timestamp,\n",
    "                            \"filename\": filename,\n",
    "                            \"folder_type\": \"raw_batches\",\n",
    "                            \"batch_number\": batch_count_raw,\n",
    "                            \"lines_count\": len(buffer_raw),\n",
    "                            \"file_size_bytes\": file_size,\n",
    "                            \"file_size_kb\": round(file_size / 1024, 2),\n",
    "                            \"processing_duration_sec\": round(duration, 3),\n",
    "                            \"status\": \"success\"\n",
    "                        }\n",
    "                        append_log_to_csv(log_entry)\n",
    "                        \n",
    "                        buffer_raw.clear()\n",
    "                        last_write_time_raw = current_time\n",
    "                        batch_count_raw += 1\n",
    "\n",
    "                if len(buffer_filtered) >= 50 or (current_time - last_write_time_filtered >= 70):\n",
    "                    if buffer_filtered:\n",
    "                      \n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        filename = f\"f_batch{batch_count_filtered}.txt\"\n",
    "                        local_path = os.path.join(filtered_batches, filename)\n",
    "                        \n",
    "                       \n",
    "                        with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(\"\\n\".join(buffer_filtered))\n",
    "                        \n",
    "                    \n",
    "                        file_size = os.path.getsize(local_path)\n",
    "                        \n",
    "                       \n",
    "                        subprocess.run([\n",
    "                            r\"E:\\hadoop\\bin\\hdfs.cmd\", \"dfs\", \"-put\", \"-f\", local_path, hdfs_filtered_dir\n",
    "                        ], check=True)\n",
    "                        \n",
    "                        \n",
    "                        duration = time.time() - start_time\n",
    "                        \n",
    "                        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        print(f\"[{timestamp}] Uploaded {filename} to HDFS folder: filtered_batches | Lines: {len(buffer_filtered)}\")\n",
    "                        \n",
    "                        log_entry = {\n",
    "                            \"timestamp\": timestamp,\n",
    "                            \"filename\": filename,\n",
    "                            \"folder_type\": \"filtered_batches\",\n",
    "                            \"batch_number\": batch_count_filtered,\n",
    "                            \"lines_count\": len(buffer_filtered),\n",
    "                            \"file_size_bytes\": file_size,\n",
    "                            \"file_size_kb\": round(file_size / 1024, 2),\n",
    "                            \"processing_duration_sec\": round(duration, 3),\n",
    "                            \"status\": \"success\"\n",
    "                        }\n",
    "                        append_log_to_csv(log_entry)\n",
    "                        \n",
    "                        buffer_filtered.clear()\n",
    "                        last_write_time_filtered = current_time\n",
    "                        batch_count_filtered += 1\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"[{timestamp}] Stream Error: {e}\")\n",
    "            time.sleep(5)\n",
    "\n",
    "reddit_thread = threading.Thread(target=stream_reddit_comments, daemon=True)\n",
    "reddit_thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7cde19",
   "metadata": {},
   "source": [
    "# SPARK STREAMING\n",
    "1. Starts a SparkSession named “RedditRealTimeProcessor”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7144341",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"] = r\"E:\\Coding\\BDA-PySpark\\spark-3.4.1-bin-hadoop3\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditRealTimeProcessor\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f752c",
   "metadata": {},
   "source": [
    "`spark.streams.awaitAnyTermination()` keeps the streaming session active and waits indefinitely for incoming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6167ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.awaitAnyTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark 3.10)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
