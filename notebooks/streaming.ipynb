{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74c0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import re\n",
    "import praw\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import sys, findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b54f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Adarsh\n",
      "[nltk_data]     Ranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ---------------- NLTK SETUP ----------------\n",
    "nltk.download('words')\n",
    "english_words = set(w.lower() for w in words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809e624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- PATHS ----------------\n",
    "# Local directories\n",
    "raw_batches = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\raw_batches\"\n",
    "filtered_batches = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\filtered_batches\"\n",
    "os.makedirs(raw_batches, exist_ok=True)\n",
    "os.makedirs(filtered_batches, exist_ok=True)\n",
    "\n",
    "# HDFS directories\n",
    "hdfs_uri = \"hdfs://localhost:9000\"\n",
    "hdfs_raw_dir = f\"{hdfs_uri}/user/adarsh/realtime_pipeline/raw_batches\"\n",
    "hdfs_filtered_dir = f\"{hdfs_uri}/user/adarsh/realtime_pipeline/filtered_batches\"\n",
    "\n",
    "HDFS_CMD = r\"E:\\hadoop\\bin\\hdfs.cmd\"\n",
    "\n",
    "# Spark checkpoint\n",
    "checkpoint_dir = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\checkpoints\"\n",
    "\n",
    "os.environ['HADOOP_USER_NAME'] = 'AdarshRanjan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e173ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reddit API connected.\n"
     ]
    }
   ],
   "source": [
    "# ---------------- REDDIT API ----------------\n",
    "reddit = praw.Reddit(\n",
    "    client_id = \"ID\",\n",
    "    client_secret = \"secret\", \n",
    "    user_agent=\"LiveRedditStream/1.0 by /u/bda_pipeline\"\n",
    ")\n",
    "print(\" Reddit API connected.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "702cbb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOCAL] Deleted 5 old batch files in E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\raw_batches\n",
      "[HDFS] Deleted existing HDFS folder hdfs://localhost:9000/user/adarsh/realtime_pipeline/raw_batches\n",
      "[LOCAL] Ensured local folder exists: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\raw_batches\n",
      "[LOCAL] Created placeholder file: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\raw_batches\\Placeholder.txt\n",
      "[HDFS] Ensured HDFS folder exists: hdfs://localhost:9000/user/adarsh/realtime_pipeline/raw_batches\n",
      "[HDFS] Uploaded placeholder file to HDFS: hdfs://localhost:9000/user/adarsh/realtime_pipeline/raw_batches\n",
      "[LOCAL] Deleted 5 old batch files in E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\filtered_batches\n",
      "[HDFS] Deleted existing HDFS folder hdfs://localhost:9000/user/adarsh/realtime_pipeline/filtered_batches\n",
      "[LOCAL] Ensured local folder exists: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\filtered_batches\n",
      "[LOCAL] Created placeholder file: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\reddit_streaming\\filtered_batches\\Placeholder.txt\n",
      "[HDFS] Ensured HDFS folder exists: hdfs://localhost:9000/user/adarsh/realtime_pipeline/filtered_batches\n",
      "[HDFS] Uploaded placeholder file to HDFS: hdfs://localhost:9000/user/adarsh/realtime_pipeline/filtered_batches\n",
      "[LOCAL] Deleted entire logs directory: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\logs\n",
      "[LOCAL] Recreated empty logs directory: E:\\Coding\\BDA-PySpark\\realtime-pipeline\\logs\n",
      "\n",
      " Cleanup and placeholder setup complete.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# ---------------- Directories ----------------\n",
    "\n",
    "log = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\logs\"\n",
    "\n",
    "# Assume you already have these variables defined:\n",
    "# raw_batches, filtered_batches, hdfs_raw_dir, hdfs_filtered_dir, HDFS_CMD\n",
    "\n",
    "# ---------------- CLEANUP AND PLACEHOLDERS ----------------\n",
    "confirm = input(\"This will delete local batch files, logs, checkpoints, and HDFS data. Continue? (y/n): \")\n",
    "\n",
    "for local_dir, hdfs_dir in [(raw_batches, hdfs_raw_dir), (filtered_batches, hdfs_filtered_dir)]:\n",
    "    # ---------------- Local batch cleanup ----------------\n",
    "    if confirm.lower() == 'y' and os.path.exists(local_dir):\n",
    "        deleted_files = 0\n",
    "        for f in os.listdir(local_dir):\n",
    "            if (f.startswith(\"r_batch\") or f.startswith(\"f_batch\")) and f.endswith(\".txt\"):\n",
    "                os.remove(os.path.join(local_dir, f))\n",
    "                deleted_files += 1\n",
    "        print(f\"[LOCAL] Deleted {deleted_files} old batch files in {local_dir}\")\n",
    "    else:\n",
    "        print(f\"[LOCAL] Skipped deletion of local files in {local_dir}\")\n",
    "\n",
    "    # ---------------- HDFS cleanup ----------------\n",
    "    if confirm.lower() == 'y':\n",
    "        try:\n",
    "            subprocess.run([HDFS_CMD, \"dfs\", \"-rm\", \"-r\", \"-skipTrash\", hdfs_dir],\n",
    "                           check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            print(f\"[HDFS] Deleted existing HDFS folder {hdfs_dir}\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"[HDFS] HDFS folder {hdfs_dir} did not exist or could not be deleted\")\n",
    "    else:\n",
    "        print(f\"[HDFS] Skipped deletion of HDFS folder {hdfs_dir}\")\n",
    "\n",
    "    # ---------------- Ensure local folder exists ----------------\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    print(f\"[LOCAL] Ensured local folder exists: {local_dir}\")\n",
    "\n",
    "    # ---------------- Create placeholder file ----------------\n",
    "    placeholder_file = os.path.join(local_dir, \"Placeholder.txt\")\n",
    "    with open(placeholder_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"placeholder\\n\")\n",
    "    print(f\"[LOCAL] Created placeholder file: {placeholder_file}\")\n",
    "\n",
    "    # ---------------- Ensure HDFS folder exists ----------------\n",
    "    subprocess.run([HDFS_CMD, \"dfs\", \"-mkdir\", \"-p\", hdfs_dir], check=True)\n",
    "    print(f\"[HDFS] Ensured HDFS folder exists: {hdfs_dir}\")\n",
    "\n",
    "    # ---------------- Upload placeholder to HDFS ----------------\n",
    "    subprocess.run([HDFS_CMD, \"dfs\", \"-put\", \"-f\", placeholder_file, hdfs_dir], check=True)\n",
    "    print(f\"[HDFS] Uploaded placeholder file to HDFS: {hdfs_dir}\")\n",
    "\n",
    "# ---------------- Clean up logs ----------------\n",
    "if confirm.lower() == 'y':\n",
    "    if os.path.exists(log):\n",
    "        try:\n",
    "            shutil.rmtree(log)\n",
    "            print(f\"[LOCAL] Deleted entire logs directory: {log}\")\n",
    "            \n",
    "            # Recreate empty directory\n",
    "            os.makedirs(log, exist_ok=True)\n",
    "            print(f\"[LOCAL] Recreated empty logs directory: {log}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[LOCAL] Failed to delete logs directory: {e}\")\n",
    "    else:\n",
    "        print(f\"[LOCAL] Logs directory doesn't exist: {log}\")\n",
    "else:\n",
    "    print(f\"[LOCAL] Skipped deletion (no recreation)\")\n",
    "\n",
    "print(\"\\n Cleanup and placeholder setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c634b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Reddit Emoji Dictionary ----------------\n",
    "emoji_dict = {\n",
    "    # Joy / happiness\n",
    "    \"😂\": \"joy\", \"🤣\": \"laugh\", \"😊\": \"happy\", \"😄\": \"smile\", \"😁\": \"grin\", \"😆\": \"laugh\",\n",
    "    \"😃\": \"smile\", \"😎\": \"cool\", \"👍\": \"like\", \"❤️\": \"love\", \"💖\": \"love\", \"💯\": \"great\",\n",
    "\n",
    "    # Sadness / disappointment\n",
    "    \"😢\": \"sad\", \"😭\": \"cry\", \"🙁\": \"sad\", \"😔\": \"disappointed\", \"😟\": \"worried\", \"😞\": \"sad\",\n",
    "    \"😩\": \"tired\", \"😫\": \"tired\",\n",
    "\n",
    "    # Anger / frustration\n",
    "    \"😡\": \"angry\", \"😠\": \"angry\", \"👿\": \"angry\", \"💀\": \"death\", \"🔥\": \"fire\", \"⚡\": \"shock\",\n",
    "\n",
    "    # Love / admiration\n",
    "    \"😍\": \"love\", \"😘\": \"kiss\", \"🥰\": \"love\", \"💘\": \"love\", \"💓\": \"love\", \"💝\": \"love\",\n",
    "\n",
    "    # Surprise / excitement\n",
    "    \"😲\": \"surprise\", \"😮\": \"surprise\", \"😳\": \"shocked\", \"😱\": \"shocked\", \"🤯\": \"mindblown\",\n",
    "\n",
    "    # Sleep / relaxation\n",
    "    \"😴\": \"sleepy\", \"💤\": \"sleep\", \"😌\": \"relieved\", \"😪\": \"sleepy\", \"😇\": \"innocent\",\n",
    "\n",
    "    # Misc positive\n",
    "    \"👏\": \"clap\", \"🤝\": \"handshake\", \"✨\": \"sparkle\", \"🌟\": \"star\", \"🎉\": \"celebration\", \"🎊\": \"celebration\",\n",
    "\n",
    "    # Misc negative\n",
    "    \"👎\": \"dislike\", \"💔\": \"heartbroken\", \"☹️\": \"sad\", \"🤢\": \"disgust\", \"🤮\": \"disgust\",\n",
    "\n",
    "    # Misc reactions\n",
    "    \"🤔\": \"thinking\", \"🙄\": \"eyeroll\", \"🤷\": \"shrug\", \"😐\": \"neutral\", \"😶\": \"silent\", \"😬\": \"nervous\"\n",
    "}\n",
    "\n",
    "def replace_emojis(text):\n",
    "    for emoji, meaning in emoji_dict.items():\n",
    "        text = text.replace(emoji, f\" {meaning} \")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808d27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- UTILITY FUNCTIONS ----------------\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ✅ REDUCED stopwords list - keep more meaningful words for topic modeling\n",
    "stop_words = {\n",
    "    \"a\", \"an\", \"the\", \"and\", \"or\", \"is\", \"are\", \"to\", \"of\", \"in\", \"that\", \"this\",\n",
    "    \"it\", \"on\", \"for\", \"with\", \"as\", \"was\", \"at\", \"by\", \"be\", \"from\", \"has\", \"have\",\n",
    "    \"u\", \"im\", \"yeah\", \"oh\"\n",
    "}\n",
    "\n",
    "def clean_raw_text(text, min_words=50, chunk_size=150):\n",
    "    \"\"\"\n",
    "    Clean text for raw batches (topic modeling):\n",
    "    - Produce multiple paragraphs from each comment for better topic coverage\n",
    "    - Minimal stopwords removal\n",
    "    - Lemmatization\n",
    "    - Keep punctuation (.!?) and sentence structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # ✅ DON'T lowercase yet - preserve proper nouns\n",
    "    original_text = text\n",
    "    \n",
    "    # Replace emojis\n",
    "    text = replace_emojis(text)\n",
    "    \n",
    "    # Remove URLs, mentions/hashtags\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[@#]\\w+\", \"\", text)\n",
    "    \n",
    "    # ✅ REMOVED: Don't remove repeated chars (important for emphasis)\n",
    "    # ✅ REMOVED: Don't remove non-ASCII (keeps accents, special chars)\n",
    "    \n",
    "    # ✅ NOW lowercase after preserving structure\n",
    "    text = text.lower()\n",
    "    \n",
    "    # ✅ Keep more punctuation for sentence structure\n",
    "    text = re.sub(r\"[^a-z\\s.,!?'-]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Fix broken apostrophes\n",
    "    text = re.sub(r\"(\\w)'m(\\w)\", r\"\\1m\\2\", text)  # ki'm → kim\n",
    "    text = re.sub(r\"(\\w)'(\\w)\", r\"\\1\\2\", text)    # dri've → drive\n",
    "\n",
    "    \n",
    "    # ✅ Fix contractions for better readability\n",
    "    text = text.replace(\"dont\", \"don't\")\n",
    "    text = text.replace(\"cant\", \"can't\")\n",
    "    text = text.replace(\"wont\", \"won't\")\n",
    "    text = text.replace(\"didnt\", \"didn't\")\n",
    "    text = text.replace(\"doesnt\", \"doesn't\")\n",
    "    text = text.replace(\"im\", \"i'm\")\n",
    "    text = text.replace(\"ive\", \"i've\")\n",
    "    text = text.replace(\"youre\", \"you're\")\n",
    "    text = text.replace(\"theyre\", \"they're\")\n",
    "    text = text.replace(\"thats\", \"that's\")\n",
    "    \n",
    "    # ✅ CHANGED: Don't remove stopwords and lemmatize immediately\n",
    "    # Keep words as-is for better topic modeling\n",
    "    words_list = [\n",
    "        w for w in text.split()\n",
    "        if len(w) > 1  # Only remove single chars\n",
    "    ]\n",
    "    \n",
    "    # If too short, return None (don't keep garbage)\n",
    "    if len(words_list) < min_words:\n",
    "        return None  # ✅ Changed from returning list to None\n",
    "    \n",
    "    # ✅ CHANGED: Keep as ONE document, don't split into chunks\n",
    "    # Topic modeling works better with complete documents\n",
    "    return [\" \".join(words_list)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64192c4e",
   "metadata": {},
   "source": [
    "# ---------------- UTILITY FUNCTIONS ----------------\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) | {\n",
    "    \"u\", \"im\", \"dont\", \"cant\", \"didnt\", \"ive\", \"yeah\", \"oh\", \"like\", \"know\",\n",
    "    \"a\", \"an\", \"the\", \"and\", \"or\", \"is\", \"are\", \"to\", \"of\", \"in\", \"that\", \"this\",\n",
    "    \"it\", \"on\", \"for\", \"with\", \"as\", \"was\", \"at\", \"by\", \"be\", \"from\", \"has\", \"have\"\n",
    "}\n",
    "\n",
    "def clean_raw_text(text, min_words=50, chunk_size=150):\n",
    "    \"\"\"\n",
    "    Clean text for raw batches (topic modeling):\n",
    "    - Produce multiple paragraphs from each comment for better topic coverage\n",
    "    - Minimal stopwords removal\n",
    "    - Lemmatization\n",
    "    - Keep punctuation (.!?)\n",
    "    \"\"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace emojis\n",
    "    text = replace_emojis(text)\n",
    "\n",
    "    # Remove URLs, mentions/hashtags, repeated chars, non-ASCII\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[@#]\\w+\", \"\", text)\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "\n",
    "    # Keep punctuation . , ! ? \n",
    "    text = re.sub(r\"[^a-z\\s.,!?]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Tokenize, remove minimal stopwords, lemmatize\n",
    "    words_list = [\n",
    "        lemmatizer.lemmatize(w)\n",
    "        for w in text.split()\n",
    "        if w not in stop_words and w in english_words\n",
    "    ]\n",
    "\n",
    "    # If too short, keep as one paragraph\n",
    "    if len(words_list) < min_words:\n",
    "        return [\" \".join(words_list)]\n",
    "\n",
    "    # Split into multiple paragraphs of chunk_size words\n",
    "    paragraphs = [\n",
    "        \" \".join(words_list[i:i+chunk_size])\n",
    "        for i in range(0, len(words_list), chunk_size)\n",
    "    ]\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ab2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filtered_text(text, min_words=30):\n",
    "    \"\"\"\n",
    "    Minimal cleaning for sentiment analysis:\n",
    "    - Keep emotion words and negations (CRITICAL!)\n",
    "    - Keep punctuation (! ? . , for sentiment)\n",
    "    - Fix contractions for readability\n",
    "    - NO stopword removal (they carry sentiment!)\n",
    "    - Keep as ONE complete comment (don't split)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic cleanup\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Replace emojis with text\n",
    "    text = replace_emojis(text)\n",
    "    \n",
    "    # Remove URLs and Reddit patterns\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"/[ru]/\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[@#]\\w+\", \"\", text)\n",
    "    \n",
    "    # Remove excessive repeated characters (loooove -> loove)\n",
    "    text = re.sub(r\"(.)\\1{3,}\", r\"\\1\\1\", text)  # Changed from 2 to 3\n",
    "    \n",
    "    # ✅ FIX CONTRACTIONS (IMPORTANT for sentiment!)\n",
    "    text = text.replace(\"dont\", \"don't\")\n",
    "    text = text.replace(\"cant\", \"can't\")\n",
    "    text = text.replace(\"wont\", \"won't\")\n",
    "    text = text.replace(\"didnt\", \"didn't\")\n",
    "    text = text.replace(\"doesnt\", \"doesn't\")\n",
    "    text = text.replace(\"isnt\", \"isn't\")\n",
    "    text = text.replace(\"arent\", \"aren't\")\n",
    "    text = text.replace(\"wasnt\", \"wasn't\")\n",
    "    text = text.replace(\"werent\", \"weren't\")\n",
    "    text = text.replace(\"hasnt\", \"hasn't\")\n",
    "    text = text.replace(\"havent\", \"haven't\")\n",
    "    text = text.replace(\"hadnt\", \"hadn't\")\n",
    "    text = text.replace(\"wouldnt\", \"wouldn't\")\n",
    "    text = text.replace(\"shouldnt\", \"shouldn't\")\n",
    "    text = text.replace(\"couldnt\", \"couldn't\")\n",
    "    text = text.replace(\"im\", \"i'm\")\n",
    "    text = text.replace(\"ive\", \"i've\")\n",
    "    text = text.replace(\"youre\", \"you're\")\n",
    "    text = text.replace(\"youve\", \"you've\")\n",
    "    text = text.replace(\"theyre\", \"they're\")\n",
    "    text = text.replace(\"theyve\", \"they've\")\n",
    "    text = text.replace(\"its\", \"it's\")\n",
    "    text = text.replace(\"thats\", \"that's\")\n",
    "    text = text.replace(\"whats\", \"what's\")\n",
    "    text = text.replace(\"heres\", \"here's\")\n",
    "    text = text.replace(\"theres\", \"there's\")\n",
    "    \n",
    "    # ✅ Fix broken apostrophes from your data\n",
    "    text = re.sub(r\"(\\w)'(\\w)\", r\"\\1\\2\", text)  # cli'mb → climb\n",
    "    \n",
    "    # ✅ KEEP PUNCTUATION - only remove extreme special chars\n",
    "    text = re.sub(r\"[^\\w\\s.,!?'-]\", \"\", text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # ✅ DON'T remove stopwords - they're important for sentiment!\n",
    "    # ✅ DON'T split into chunks - keep complete comment\n",
    "    \n",
    "    # Check minimum length\n",
    "    word_count = len(text.split())\n",
    "    if word_count < min_words:\n",
    "        return None  # Skip very short comments\n",
    "    \n",
    "    # Return as single document\n",
    "    return [text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ea9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch_number(folder, prefix=\"batch\", suffix=\".txt\"):\n",
    "    \"\"\"Get the next batch number dynamically based on existing files.\"\"\"\n",
    "    existing = [f for f in os.listdir(folder) if f.startswith(prefix) and f.endswith(suffix)]\n",
    "    if not existing:\n",
    "        return 1\n",
    "    numbers = []\n",
    "    for f in existing:\n",
    "        try:\n",
    "            num = int(re.findall(r'\\d+', f)[0])\n",
    "            numbers.append(num)\n",
    "        except:\n",
    "            continue\n",
    "    return max(numbers) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759beb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-27 00:46:08] Uploaded f_batch1.txt to HDFS folder: filtered_batches | Lines: 10\n",
      "[2025-10-27 00:46:28] Uploaded r_batch1.txt to HDFS folder: raw_batches | Lines: 3\n",
      "[2025-10-27 00:47:18] Uploaded f_batch2.txt to HDFS folder: filtered_batches | Lines: 7\n",
      "[2025-10-27 00:47:58] Uploaded r_batch2.txt to HDFS folder: raw_batches | Lines: 4\n",
      "[2025-10-27 00:48:29] Uploaded f_batch3.txt to HDFS folder: filtered_batches | Lines: 17\n",
      "[2025-10-27 00:49:29] Uploaded r_batch3.txt to HDFS folder: raw_batches | Lines: 10\n",
      "[2025-10-27 00:49:39] Uploaded f_batch4.txt to HDFS folder: filtered_batches | Lines: 8\n",
      "[2025-10-27 00:50:49] Uploaded f_batch5.txt to HDFS folder: filtered_batches | Lines: 20\n",
      "[2025-10-27 00:51:00] Uploaded r_batch4.txt to HDFS folder: raw_batches | Lines: 10\n",
      "[2025-10-27 00:51:59] Uploaded f_batch6.txt to HDFS folder: filtered_batches | Lines: 8\n",
      "[2025-10-27 00:52:30] Uploaded r_batch5.txt to HDFS folder: raw_batches | Lines: 8\n",
      "[2025-10-27 00:53:10] Uploaded f_batch7.txt to HDFS folder: filtered_batches | Lines: 12\n",
      "[2025-10-27 00:54:00] Uploaded r_batch6.txt to HDFS folder: raw_batches | Lines: 8\n",
      "[2025-10-27 00:54:20] Uploaded f_batch8.txt to HDFS folder: filtered_batches | Lines: 17\n",
      "[2025-10-27 00:55:30] Uploaded r_batch7.txt to HDFS folder: raw_batches | Lines: 12\n",
      "[2025-10-27 00:55:34] Uploaded f_batch9.txt to HDFS folder: filtered_batches | Lines: 20\n",
      "[2025-10-27 00:56:44] Uploaded f_batch10.txt to HDFS folder: filtered_batches | Lines: 15\n",
      "[2025-10-27 00:57:00] Uploaded r_batch8.txt to HDFS folder: raw_batches | Lines: 9\n",
      "[2025-10-27 00:57:56] Uploaded f_batch11.txt to HDFS folder: filtered_batches | Lines: 14\n",
      "[2025-10-27 00:58:33] Uploaded r_batch9.txt to HDFS folder: raw_batches | Lines: 9\n",
      "[2025-10-27 00:59:11] Uploaded f_batch12.txt to HDFS folder: filtered_batches | Lines: 12\n",
      "[2025-10-27 01:00:10] Uploaded r_batch10.txt to HDFS folder: raw_batches | Lines: 9\n",
      "[2025-10-27 01:00:22] Uploaded f_batch13.txt to HDFS folder: filtered_batches | Lines: 10\n",
      "[2025-10-27 01:01:34] Uploaded f_batch14.txt to HDFS folder: filtered_batches | Lines: 13\n",
      "[2025-10-27 01:01:43] Uploaded r_batch11.txt to HDFS folder: raw_batches | Lines: 7\n",
      "[2025-10-27 01:02:42] Uploaded f_batch15.txt to HDFS folder: filtered_batches | Lines: 16\n",
      "[2025-10-27 01:03:13] Uploaded r_batch12.txt to HDFS folder: raw_batches | Lines: 10\n",
      "[2025-10-27 01:03:52] Uploaded f_batch16.txt to HDFS folder: filtered_batches | Lines: 13\n",
      "[2025-10-27 01:04:42] Uploaded r_batch13.txt to HDFS folder: raw_batches | Lines: 9\n",
      "[2025-10-27 01:05:00] Uploaded f_batch17.txt to HDFS folder: filtered_batches | Lines: 11\n",
      "[2025-10-27 01:06:10] Uploaded f_batch18.txt to HDFS folder: filtered_batches | Lines: 16\n",
      "[2025-10-27 01:06:14] Uploaded r_batch14.txt to HDFS folder: raw_batches | Lines: 10\n",
      "[2025-10-27 01:07:21] Uploaded f_batch19.txt to HDFS folder: filtered_batches | Lines: 12\n",
      "[2025-10-27 01:07:44] Uploaded r_batch15.txt to HDFS folder: raw_batches | Lines: 7\n",
      "[2025-10-27 01:08:31] Uploaded f_batch20.txt to HDFS folder: filtered_batches | Lines: 12\n",
      "[2025-10-27 01:09:15] Uploaded r_batch16.txt to HDFS folder: raw_batches | Lines: 6\n",
      "[2025-10-27 01:09:41] Uploaded f_batch21.txt to HDFS folder: filtered_batches | Lines: 16\n",
      "[2025-10-27 01:10:56] Uploaded r_batch17.txt to HDFS folder: raw_batches | Lines: 10\n",
      "[2025-10-27 01:10:59] Uploaded f_batch22.txt to HDFS folder: filtered_batches | Lines: 16\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- LOG FUNCTION ----------------\n",
    "def append_log_to_csv(log_entry):\n",
    "    \"\"\"Append single log entry to CSV\"\"\"\n",
    "    log_csv_path = r\"E:\\Coding\\BDA-PySpark\\realtime-pipeline\\logs\\upload_logs.csv\"\n",
    "    \n",
    "    os.makedirs(os.path.dirname(log_csv_path), exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(log_csv_path):\n",
    "        df = pd.DataFrame([log_entry])\n",
    "        df.to_csv(log_csv_path, index=False)\n",
    "    else:\n",
    "        df = pd.DataFrame([log_entry])\n",
    "        df.to_csv(log_csv_path, mode='a', header=False, index=False)\n",
    "\n",
    "# ---------------- REDDIT STREAMING ----------------\n",
    "buffer_raw, buffer_filtered = [], []\n",
    "last_write_time_raw, last_write_time_filtered = time.time(), time.time()\n",
    "batch_count_raw = get_next_batch_number(raw_batches, prefix=\"r_batch\")\n",
    "batch_count_filtered = get_next_batch_number(filtered_batches, prefix=\"f_batch\")\n",
    "\n",
    "def stream_reddit_comments():\n",
    "    global buffer_raw, buffer_filtered\n",
    "    global last_write_time_raw, last_write_time_filtered\n",
    "    global batch_count_raw, batch_count_filtered\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            for comment in reddit.subreddit(\"all\").stream.comments(skip_existing=True):\n",
    "                text = comment.body.replace(\"\\n\", \" \").strip()\n",
    "                \n",
    "                lines_raw = clean_raw_text(text)\n",
    "                lines_filtered = clean_filtered_text(text)\n",
    "\n",
    "                if lines_raw:\n",
    "                    buffer_raw.extend(lines_raw)\n",
    "                if lines_filtered:\n",
    "                    buffer_filtered.extend(lines_filtered)\n",
    "\n",
    "                current_time = time.time()\n",
    "\n",
    "                # ---------------- Raw batch writing ----------------\n",
    "                if len(buffer_raw) >= 50 or (current_time - last_write_time_raw >= 90):\n",
    "                    if buffer_raw:\n",
    "                        # Start timing\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        filename = f\"r_batch{batch_count_raw}.txt\"\n",
    "                        local_path = os.path.join(raw_batches, filename)\n",
    "                        \n",
    "                        # ✅ WRITE FILE FIRST\n",
    "                        # In your batch writing:\n",
    "                        with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(\"\\n\\n\".join(buffer_raw))  # Double newline between comments\n",
    "\n",
    "                        \n",
    "                        # ✅ THEN GET FILE SIZE (file now exists)\n",
    "                        file_size = os.path.getsize(local_path)\n",
    "                        \n",
    "                        # Upload to HDFS\n",
    "                        subprocess.run([\n",
    "                            r\"E:\\hadoop\\bin\\hdfs.cmd\", \"dfs\", \"-put\", \"-f\", local_path, hdfs_raw_dir\n",
    "                        ], check=True)\n",
    "                        \n",
    "                        # Calculate duration\n",
    "                        duration = time.time() - start_time\n",
    "                        \n",
    "                        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        print(f\"[{timestamp}] Uploaded {filename} to HDFS folder: raw_batches | Lines: {len(buffer_raw)}\")\n",
    "                        \n",
    "                        log_entry = {\n",
    "                            \"timestamp\": timestamp,\n",
    "                            \"filename\": filename,\n",
    "                            \"folder_type\": \"raw_batches\",\n",
    "                            \"batch_number\": batch_count_raw,\n",
    "                            \"lines_count\": len(buffer_raw),\n",
    "                            \"file_size_bytes\": file_size,\n",
    "                            \"file_size_kb\": round(file_size / 1024, 2),\n",
    "                            \"processing_duration_sec\": round(duration, 3),\n",
    "                            \"status\": \"success\"\n",
    "                        }\n",
    "                        append_log_to_csv(log_entry)\n",
    "                        \n",
    "                        buffer_raw.clear()\n",
    "                        last_write_time_raw = current_time\n",
    "                        batch_count_raw += 1\n",
    "\n",
    "                # ---------------- Filtered batch writing ----------------\n",
    "                if len(buffer_filtered) >= 50 or (current_time - last_write_time_filtered >= 70):\n",
    "                    if buffer_filtered:\n",
    "                        # Start timing\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        filename = f\"f_batch{batch_count_filtered}.txt\"\n",
    "                        local_path = os.path.join(filtered_batches, filename)\n",
    "                        \n",
    "                        # ✅ WRITE FILE FIRST\n",
    "                        with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(\"\\n\".join(buffer_filtered))\n",
    "                        \n",
    "                        # ✅ THEN GET FILE SIZE (file now exists)\n",
    "                        file_size = os.path.getsize(local_path)\n",
    "                        \n",
    "                        # Upload to HDFS\n",
    "                        subprocess.run([\n",
    "                            r\"E:\\hadoop\\bin\\hdfs.cmd\", \"dfs\", \"-put\", \"-f\", local_path, hdfs_filtered_dir\n",
    "                        ], check=True)\n",
    "                        \n",
    "                        # Calculate duration\n",
    "                        duration = time.time() - start_time\n",
    "                        \n",
    "                        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        print(f\"[{timestamp}] Uploaded {filename} to HDFS folder: filtered_batches | Lines: {len(buffer_filtered)}\")\n",
    "                        \n",
    "                        log_entry = {\n",
    "                            \"timestamp\": timestamp,\n",
    "                            \"filename\": filename,\n",
    "                            \"folder_type\": \"filtered_batches\",\n",
    "                            \"batch_number\": batch_count_filtered,\n",
    "                            \"lines_count\": len(buffer_filtered),\n",
    "                            \"file_size_bytes\": file_size,\n",
    "                            \"file_size_kb\": round(file_size / 1024, 2),\n",
    "                            \"processing_duration_sec\": round(duration, 3),\n",
    "                            \"status\": \"success\"\n",
    "                        }\n",
    "                        append_log_to_csv(log_entry)\n",
    "                        \n",
    "                        buffer_filtered.clear()\n",
    "                        last_write_time_filtered = current_time\n",
    "                        batch_count_filtered += 1\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"[{timestamp}] Stream Error: {e}\")\n",
    "            time.sleep(5)\n",
    "\n",
    "# Start the streaming thread\n",
    "reddit_thread = threading.Thread(target=stream_reddit_comments, daemon=True)\n",
    "reddit_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7144341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- SPARK STREAMING ----------------\n",
    "os.environ[\"SPARK_HOME\"] = r\"E:\\Coding\\BDA-PySpark\\spark-3.4.1-bin-hadoop3\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditRealTimeProcessor\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6167ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.awaitAnyTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark 3.10)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
